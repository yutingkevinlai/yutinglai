<!DOCTYPE html>
<html lang="en">
<head>
	<title>Kevin Lai</title>
	<meta charset="UTF-8" />
	<meta name="description" content="Kevin Lai, Graduate Student Researcher" />
	<meta name="robots" content="index,follow" />
	<meta name="viewport" content="width=device-width" />
	<link rel="stylesheet" media="all" href="s.css" />
</head>
<body><div id="c"><div id="h">
<h1><a href="../" title="Home">Kevin Lai</a></h1>
<div id="n"><ul>
	<li><a href="../portfolio" title="Portfolio" class="a">Portfolio</a></li>
	<li><a href="../cv" title="Curriculum Vitae (CV)">cv	</a></li>
	<li><a href="../publications" title="Publications">Publications</a></li>
	<li><a href="../teaching" title="Teaching">Teaching</a></li>
	<li><a href="../resources" title="Shared Resources">Resources</a></li>
	<li><a href="../contact" title="Contact">Contact</a></li></ul>
	</div>
</div>
<div id="cm">


<p>My research interest include the design and control of robotic systems,
sensor fusion, and image-guided robot navigation.
	<!-- This includes a focus on automated intraocular robotic surgical systems and  --></p>

<div id="port">
<h2>Current Projects</h2>

<a href="./IOPControl" title="IOP Control" class="pp">
	<img src="./i/iop_control.png" alt="">
	<p>
		<strong>Intraocular Pressure Control</strong><br />
		<span>08/2022 &#8211; Present</span><br />
		Intraocular pressure control on the anterior chamber of the pig eye.</p>
	<p>Pressure Control / OCT Sensing / Surgical Automation</p>
</a>
<a href="./Ascan" title="Ascan" class="pp">
	<img src="./Ascan/ascan.JPG" alt="">
	<p>
		<strong>Complete Capsule Mapping</strong><br />
		<span>02/2023 &#8211; Present</span><br />
		Visualize the lens equatorial region through enhance OCT sensing.</p>
	<p>Image Processing / OCT Sensing / Surgical Automation</p>
</a>


<!-- <a href="portfolio/LensExtraction/" title="Lens Extraction" class="pp">
	<img src="portfolio/LensExtraction/overview.png" alt="">
	<p>
		<strong>Cataract Lens Extraction</strong><br />
		<span>09/2021</span><br />
		Automated OCT-guided robotic cataract lens extraction on the pig eye.</p>
	<p>Autonomous Guidance / OCT Guidance / Surgical Robotics</p>
</a> -->
<!-- <a href="/CollaborativeRobot" title="Collaborative Robot" class="pp">
	<img src="portfolio/CollaborativeRobot/overview.png" alt="">
	<p>
		<strong>Collaborative Robot</strong><br />
		<span>09/2021</span><br />
		Iterative learning control for 6-axis robots collaboration</p>
	<p>Iterative Learning Control / </p>
</a> -->


<!-- <div class="res">
	<img src="./i/iriss_v3.gif">
	<h4>Robotic Trajectory Planning for Cataract Eye Surgery</h4>
	<p>Cataract surgery requires extremely precise manipulation, which motivates us to use a small-size manipulator from
		<a href="https://www.mecademic.com/products/Meca500-small-robot-arm">Meca500</a>.
		The robot is able to reach a 0.005(mm) repeatability which suits our needs.
		We parameterized the eye model to fits different shapes of ex-vivo pig eyes or human eyes.
		We then do the trajectory planning inside the lens for the anterior capsule and posterior capsule.
		At the same time, the tool is pivoted at the remote center of motion (RCM) to reduce any rupture of the eye.
		The current stage of the project is to validate the whole cataract surgery process in simulation before experimentally run with any real eyes.
	</p>
</div> -->

<!-- <div class="res">
	<img src="./i/itri.png">
	<h4>Iterative Learning Control (ILC) for Manipulator Collaboration</h4>
	<p>Manipulators are common in industrial applications. It can perform repetative tasks precisely.
		However, for aerospace applications, it requires higher precision on the manufacture of aircrafts and rockets.
		To achieve this, errors on the position should be minimized as well as the contacting force between two agents to avoid any deformation.
		In this study, we focus on reducing the position error and internal force between two 6-joint manipulators by iterative learning control (ILC).
	</p>
</div> -->
<!--
<div class="res">
	<img src="i/resIRISS.jpg" />
	<h4>Partially Automated Lens Extraction in Ex Vivo Pig Eyes</h4>
	<p>With the development of laser-assisted platforms, the outcomes of cataract
surgery have been improved by automating several procedures. The cataract-extraction
step continues to be manually performed, but due to deficiencies in sensing capabilities,
surgical complications such as posterior capsule rupture and incomplete cataract removal remain.
An optical coherence tomography (OCT) system was integrated into the intraocular
robotic interventional surgical system (IRISS) robot.
The OCT data was used for preoperative planning and intraoperative intervention in a series of automated procedures.
Real-time intervention allowed the surgeon to evaluate the progress and override the operation.
The developed system was validated by performing lens extraction on 30 <em>ex vivo</em> pig eyes.
Complete lens extraction was achieved on 25 eyes, and ‘‘almost complete’’ extraction
was achieved on the remainder due to an inability to image small lens particles behind the iris.
No capsule rupture was found.</p>
	<p class="pub">Selected Publication:<br />
	<strong>Semiautomated Optical Coherence Tomography-Guided Robotic Surgery for Porcine Lens Removal</strong>, C.W. Chen, A.A. Francone, <u>M.J. Gerber</u>, Y.H. Lee, A. Govetto, T.C. Tsao, and J.P. Hubschman, <em>Journal of Cataract &amp; Refractive Surgery</em>, vol. 45(11), pp. 1665&#8211;1669, 2019, <a href="https://doi.org/10.1016/j.jcrs.2019.06.020" title="DOI">DOI</a> | <a href="../papers/paper_2019iriss.pdf" title="PDF">PDF</a></p>
</div>
-->
<h2>Selected Past Projects</h2>

<a href="./PCPolishing" title="PC Polishing" class="pp">
	<img src="./i/pc_polishing.png" alt="">
	<p>
		<strong>Posterior Capsule (PC) Polishing</strong><br />
		<span>12/2021</span><br />
		Automated OCT-guided robotic PC polishing on ex-vivo pig eyes.</p>
	<p>Autonomous Guidance / OCT Guidance / Surgical Robotics</p>
</a>

<a href="./TrafficLightDetection" title="Traffic Light Detection" class="pp">
	<img src="./i/traffic_light.gif" alt="">
	<p>
		<strong>Traffic Light Detection</strong><br />
		<span>01/2019</span><br />
		Vision-guided traffic light detection and recognition on a vehicle.</p>
	<p>Autonomous Vehicle / Sensor Fusion / Image Processing </p>
</a>


<!-- <h2>Small Side Projects</h2>

<a href="./DualPumpSystem" title="Dual Pump System" class="pp">
	<img src="./i/pumpOverviewResized.png" alt="">
	<p>
		<strong>Dual Pump System</strong><br />
		<span>06/2023</span><br />
		A small pump system that can perform fast dynamnics for IOP control.</p>
	<p>Solidworks / 3D Printing / Circuit Design </p>
</a> -->



<!-- <a href="portfolio/SLAMObjectRemoval/" title="SLAM Object Removal" class="pp">
	<img src="./i/slam_object_paths.png" alt="">
	<p>
		<strong>SLAM with Moving Object Removal</strong><br />
		<span>12/2018</span><br />
		Simultaneous localization and mapping with moving object removal</p>
	<p>Autonomous Vehicle / Sensor Fusion / Point Cloud Processing </p>
</a> -->

<!-- <div class="res">
	<img src="./i/slam_object_paths.png" />
	<h4>SLAM with Moving Object Removal</h4>
	<p>The goal of this poject is to construct a static map in the real world using SLAM.
		The result of SLAM would contain the trajectories of moving objects, which will form clear paths in the constructed map.
		These objects are temporary which will not exist in later lidar scans when doing localization.
		This will cause localization problem if the vehicle constructed the map from a relatively dynamical scene.
		To remove these paths, we proposed an algorithm to remove moving objects and retain only static objects in lidar scans prior to map construction.
	<a href="https://github.com/yutingkevinlai/velodyne_slam" title="Github">Github</a> | <a href="./docs/slam_moving_object_removal.pdf" title="PDF">PDF</a>
	</p>
</div> -->

<!-- <a href="portfolio/AnomalyDetection/" title="Anomaly Detection" class="pp">
	<img src="./i/anomaly_detection.PNG" alt="">
	<p>
		<strong>Industrial Defect Inspection using Generative Adversarial Networks</strong><br />
		<span>06/2018</span><br />
		Anomaly detection on manufacturing line</p>
	<p>Anomaly Detection / Generative Adversarial Network / Image Processing </p>
</a> -->

<!-- <div class="res">
	<img src="./i/anomaly_detection.PNG" />
	<h4>Industrial Defect Inspection using Generative Adversarial Networks</h4>
	<p>Defect inspection is one of most importants stage in product manufacturing process. Computer
image processing technique is the most commonly used method to detect defects. Usually the methods
need to define defects, build a mathematical model, and set a reasonable threshold value. This requires
human operators to annotate defects in advance, and a large amount of defect dataset to ensure that the
threshold value can achieve a low false positive rate while maintaining a reasonable false positive rate.
Since industrial image datasets are mostly sparse in defects, it is hard for automated optical inspection
(AOI) machines to inspect all defects effectively.
We proposed a novel framework for industrial anomaly detection in an unsupervised manner which does not need tedious annotation in advance.
The results show that GANs are able to capture arbitrary and structural industrial images.
Moreover, extensive experimental results on real industrial datasets show that the proposed method can successfully construct the surface texture
pattern generator. By transforming the image through the generator to the corresponding latent space,
the defects can be subsequently separated effectively without annotation in a large amount of training
data. Thus, the proposed method only need defect-free images in the training stage, that is, it belongs
to an one-class classification task.
	</p>
	<p class="pub">Selected Publication:<br />
	<strong>A Texture Generation Approach for Detection of Novel Surface Defects</strong>, <u>Y.T. Lai</u> and J.S. Hu, <em>IEEE International Conference on System, Man, and Cybernetics</em>, Oct 2018, <a href="https://doi.org/10.1109/SMC.2018.00736" title="DOI">DOI</a> | <a href="./docs/2018_surface_inspection.pdf" title="PDF">PDF</a></p>
</div> -->

</div><!--#cm-->
<div id="f">
	<span>&copy;2025 Kevin Lai</span>
	<span><strong><a href="mailto:yutingkevinlai@gmail.com">yutingkevinlai@gmail.com</a></strong></span>
	<span>1540 Boelter Hall, 420 Westwood Plaza, Los Angeles, CA</span>
	</div>
</div><!--#c-->
</body></html>
